{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Execution Strategies with Research Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to Research Module\n",
    "    * Basic example\n",
    "        * 1 pipeline with fixed parameters\n",
    "             * creating research\n",
    "             * running several repetitions of an experiment\n",
    "             * viewing research results\n",
    "             * saving and loading research\n",
    "    * Runnung experiments with different parameters aka grid\n",
    "        * 1 pipeline with variable parameters\n",
    "             * creating and viewing grids\n",
    "             * viewing filtered research results\n",
    "             \n",
    "             \n",
    "2. Complex Execution Strategies with Research Module (**You are here**)\n",
    "    * Reducing extra dataset loads\n",
    "        * 1 pipeline with root and branch + grid\n",
    "    * More complex execution strategies\n",
    "        * 2 pipelines, train & test + function + root&branch + grid\n",
    "            * adding test pipeline\n",
    "            * defining results recording frequency aka execute='%n'\n",
    "            * adding functions\n",
    "3. \n",
    "    * Cross-validation\n",
    "\n",
    "    * Performance\n",
    "        * execution tasks managing\n",
    "    * Combining it all together\n",
    "        * Super complex Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing extra dataset loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from tensorflow import logging\n",
    "# logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from batchflow import Pipeline, B, C, V\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow.models.tf import VGG7, VGG16\n",
    "from batchflow.research import Research, Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "ITERATIONS=10\n",
    "REPETITIONS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()\n",
    "train_root = mnist.train.p.run(BATCH_SIZE, shuffle=True, n_epochs=None, lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Option('layout', ['cna', 'can']) * Option('bias', [True, False])\n",
    "        \n",
    "model_config={\n",
    "    'inputs': dict(images={'shape': (28, 28, 1)},\n",
    "                   labels={'classes': 10, 'transform': 'ohe', 'name': 'targets'}),\n",
    "    'initial_block/inputs': 'images',\n",
    "    'body/block/layout': C('layout'),\n",
    "    'common/conv/use_bias': C('bias'),\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('loss')\n",
    "            .init_model('dynamic', VGG7, 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', fetches='loss',\n",
    "                         feed_dict={'images': B('images'),\n",
    "                                    'labels': B('labels')},\n",
    "                         save_to=V('loss', mode='w'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_root + train_template, variables='loss', name='train')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=4, n_iters=10, name='research', bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each experiment can be divided into 2 stages: root stage that is roughly same for all experiments (for example data loading and preprocessing) and branch stage that varies. If data preprocessing takes significant time one can use the batches generated on a root stage to feed to several branches that belong to different experiments. \n",
    "\n",
    "To do so, one should pass *root* and *branch* parameters to *add_pipeline()* and define number of branches per root via *branches* parameter of *run()*.\n",
    "\n",
    "A root with corresponding branches is called a **job**. Note that different roots produce different batches due to shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(root=train_root, branch=train_template, variables='loss', name='train')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=4, n_iters=10, branches=8, name='no_extra_dataload_research', bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example we use only 10 iterations to make the effect of reduced dataset load more visible.\n",
    "We can see that the numbers of results entries is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex execution strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Option('model', [VGG7]) * Option('layout', ['cna', 'can']) #* Option('bias', [True, False])\n",
    "\n",
    "model_config={\n",
    "    #'session/config': tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.45)),\n",
    "    'inputs': dict(images={'shape': (28, 28, 1)},\n",
    "                   labels={'classes': 10, 'transform': 'ohe', 'name': 'targets'}),\n",
    "    'initial_block/inputs': 'images',\n",
    "    'body/block/layout': C('layout'),\n",
    "    #'common/conv/use_bias': C('bias'),\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('train_loss')\n",
    "            .init_model('dynamic', C('model'), 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', fetches='loss',\n",
    "                         feed_dict={'images': B('images'),\n",
    "                                    'labels': B('labels')},\n",
    "                         save_to=V('train_loss', mode='w'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = mnist.test.p.run(BATCH_SIZE, shuffle=True, n_epochs=1, lazy=True) #Note  n_epochs=1\n",
    "test_template = (Pipeline()\n",
    "                 .init_variable('test_loss')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                               fetches='loss',\n",
    "                               feed_dict={'images': B('images'),\n",
    "                                          'labels': B('labels')},\n",
    "                               save_to=V('test_loss', mode='w')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EXECUTE_FREQ = '%5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(root=train_root, branch=train_template, variables='train_loss', name='train')\n",
    "            .add_pipeline(root=test_root, branch=test_template, variables='test_loss', name='test',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train') # Note run=True\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_iters=ITERATIONS, name='my_research', bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use .add_pipeline with run=False (which is the default value) this means that the pipeline is executed batch-wise and an 'iteration' refers to processing a single batch. If we use run=True than the pipeline is executed on the whole dataset and an 'iteration' is rather an epoch.\n",
    "\n",
    "When we wish to evaluate the models performance naturally we want to execute the test pipeline on the whole validation set \n",
    "\n",
    "Here for each config from grid the train pipeline's gen_batch() is run n_iters times. After each 100-th train's gen_batch() the whole test pipeline is run and model's loss on test set is evaluated. Gathering test set loss might seem rather useless, we do it solely to demonstrate how to control multiple pipeline execution, and we will soon move to a more interesting case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results(iterations=[4,9])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (Pipeline()\n",
    "                 .init_variable('predictions')\n",
    "                 .init_variable('metrics')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                               fetches='predictions',\n",
    "                               feed_dict={'images': B('images'),\n",
    "                                          'labels': B('labels')},\n",
    "                               save_to=V('predictions'))\n",
    "                .gather_metrics('class', targets=B('labels'), predictions=V('predictions'), \n",
    "                                fmt='logits', axis=-1, save_to=V('metrics')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(iteration, experiment, pipeline):\n",
    "    pipeline = experiment[pipeline].pipeline\n",
    "    metrics = pipeline.get_variable('metrics')\n",
    "    return metrics.evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(root=train_root, branch=train_template, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(root=test_root, branch=test_template, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .add_function(get_accuracy, returns='accuracy', name='test_accuracy_fn', execute=TEST_EXECUTE_FREQ, pipeline='test_ppl')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=1, n_iters=ITERATIONS, name='my_research', bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research my_research_31 is starting...\n",
      "Distributor has 2 jobs with 10 iterations. Totally: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:55<00:00,  2.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.research.research.Research at 0x1df08efac18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_on_root(iteration, experiments):\n",
    "    print(\"Running configs\", iteration)\n",
    "    \n",
    "research = (Research()\n",
    "            .add_function(function_on_root, execute=1, on_root=True)\n",
    "            .add_pipeline(root=train_root, branch=train_template, variables='train_loss', name='train')\n",
    "            .add_pipeline(root=test_root, branch=test_template, name='test',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train')\n",
    "            .add_function(get_accuracy, returns='accuracy', name='test_accuracy', execute=TEST_EXECUTE_FREQ, pipeline='test')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=2, n_iters=ITERATIONS, branches=2, name='my_research', bar=True)\n",
    "#research.run(n_reps=1, n_iters=100, workers=2, branches=2, gpu=[2, 4, 5, 6], name='my_research', bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each worker starts in a separate process and performs one or several jobs assigned to it. Moreover if several GPU's are accessible one can pass indices of GPUs to use via *gpu* parameter.\n",
    "\n",
    "timeout - minutes, default=5\n",
    "trials default=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research my_research_32 is starting...\n",
      "Distributor has 8 jobs with 10 iterations. Totally: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [02:24<00:00,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.research.research.Research at 0x1df06944ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_config={\n",
    "    'session/config': tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.45)),\n",
    "    'inputs': dict(images={'shape': (28, 28, 1)},\n",
    "                   labels={'classes': 10, 'transform': 'ohe', 'name': 'targets'}),\n",
    "    'initial_block/inputs': 'images',\n",
    "    'body/block/layout': C('layout'),\n",
    "    #'common/conv/use_bias': C('bias'),\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('train_loss')\n",
    "            .init_model('dynamic', C('model'), 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', fetches='loss',\n",
    "                         feed_dict={'images': B('images'),\n",
    "                                    'labels': B('labels')},\n",
    "                         save_to=V('train_loss', mode='w'))\n",
    ")\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(root=train_root, branch=train_template, variables='train_loss', name='train_ppl',\n",
    "                          dump=TEST_EXECUTE_FREQ)\n",
    "            .add_pipeline(root=test_root, branch=test_template, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .add_function(get_accuracy, returns='accuracy', name='test_accuracy_fn', \n",
    "                          execute=TEST_EXECUTE_FREQ, dump=TEST_EXECUTE_FREQ,\n",
    "                          pipeline='test_ppl')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=4, n_iters=ITERATIONS, name='my_research', bar=True, workers=2, gpu=[0], timeout=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
