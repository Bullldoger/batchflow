{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Research Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces Research functionality of batchflow.\n",
    "\n",
    "Research class allows easily experimenting with models parameters and entire test and train workflow configurations as well as saving and loading results of experiments in a unified form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction **<- You are here**\n",
    "    * Basic example\n",
    "        * 1 pipeline with fixed parameters\n",
    "            * creating research\n",
    "            * running several repetitions of an experiment\n",
    "            * viewing research results\n",
    "            * saving and loading research\n",
    "    * Runnung experiments with different parameters aka grid\n",
    "        * 1 pipeline with variable parameters\n",
    "            * creating and viewing grids\n",
    "            * viewing filtered research results\n",
    "    * More complex execution strategies\n",
    "        * 2 pipelines, train & test + function + grid\n",
    "            * adding test pipeline\n",
    "            * defining pipeline execution frequency\n",
    "            * adding functions\n",
    "\n",
    "1. Advanced topics\n",
    "    * Reducing extra dataset loads\n",
    "        * 1 pipeline with root and branch + grid\n",
    "    * Performance\n",
    "        * execution tasks managing\n",
    "    * Cross-validation\n",
    "\n",
    "1. Combining it all together\n",
    "    * Super complex Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some useful imports and constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from batchflow import Pipeline, B, C, V\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow.models.tf import VGG7, VGG16\n",
    "from batchflow.research import Research, Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "ITERATIONS=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple pipeline that loads some MNIST data, and trains a VGG7 model on it. It also saves the loss on each iteration in a pipeline variable. Let's call it an experiment\n",
    "\n",
    "We call a lazy version of pipeline's `run` method to define batch size to use. We pass `n_epochs=None`, because the duration of our experiment will be controlled by Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()\n",
    "train_root = mnist.train.p.run_later(BATCH_SIZE, shuffle=True, n_epochs=None)\n",
    "\n",
    "model_config={\n",
    "    'inputs/images/shape': (28, 28, 1),\n",
    "    'inputs/labels/classes': 10,\n",
    "    'inputs/labels/name': 'targets',\n",
    "    'initial_block/inputs': 'images',\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('loss')\n",
    "            .init_model('dynamic', VGG7, 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpliest thing we can do with Research is running this experiment several times to see how loss'es dynamics changes from run to run.\n",
    "\n",
    "To do this we define a Research object and add the pipeline to it. We call `add_pipeline` method and pass the `train_ppl` as the first parameter. The `variables` parameter gets a string or a list of strings that indicate which pipeline variables will be monitored by Research and written to research results on each iteration. We also provide `name` that will be written to results to indicate from where actually these results come.\n",
    "\n",
    "`logging=True` adds additional logging of the pipeline execution to log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train_ppl', logging=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run this Research.\n",
    "\n",
    "We call a Research object's `run` method and we pass following parameters:\n",
    "* `n_iters` - how many iterations will the experiment consist of. Each iteration here consists of processing a single batch\n",
    "* `n_reps` - how many times we run our experiment\n",
    "* `bar` - to provide a tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research vgg7_research is starting...\n",
      "Distributor has 4 jobs with 1000 iterations. Totally: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████                        | 2773/4000 [40:31<17:55,  1.14it/s]"
     ]
    }
   ],
   "source": [
    "research.run(n_reps=4, n_iters=ITERATIONS, name='vgg7_research', bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each research is assigned with a `name` argument provided to `run`.\n",
    "Results of the research and its log are saved in a folder with the same name in the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for name in os.listdir():\n",
    "    print(\"{}\\t{}\".format('Dir' if os.path.isdir(name) else 'File', name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the experiments' results we call `load_result` method which returns a Pandas Dataframe. We can see that it has 4 collumns which contain \n",
    "* repetition - The number of experiment run\n",
    "* iteration - the number of iteration\n",
    "* name - the source of the variable\n",
    "* loss - this is the variable name that we provided to `add_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now draw a nice plot showing our loss dynamics on each experiment repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pivot(index='iteration', columns='repetition', values='loss').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned to run multiple repetitions of a single experiment with Research. \n",
    "\n",
    "We can also run several experiments with different parameters in one research. Suppose we want to compare the performance of VGG7 and VGG16 models with different layouts ('convolution-normalization-activation' vs 'convolution-activation-normalization') without bias and we would also like to check VGG7 preformance with bias and default 'cna' layout.\n",
    "\n",
    "To do so we define a grid of parameters as follows. \n",
    "We define an Option that consists of the parameter to vary and a list of values that we want to try in our research. Each parameter value defines a node in a parameter grid. We can add grids to unite the nodes and multiply grids to get Cartesian product.\n",
    "\n",
    "`grid.gen_configs()` returns a generator that yields one node (that is, a single experiment specification) at a time. Printing a list of all nodes shows us all experiment modifications in a dict-like mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = (Option('layout', ['cna', 'can']) * Option('model', [VGG7, VGG16]) * Option('bias', [False])\n",
    "        +  Option('layout', ['cna']) * Option('bias', [True]) * Option('model', [VGG7]))\n",
    "list(grid.gen_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now update `model_config` so that it could read the values from the grid.\n",
    "\n",
    "We pass named expressions as parameters values with names from our parameter grid. \n",
    "We define layout and bias in the model config and we pass model type as a named expression to `init_model` method of the pipeline (config option named expression `C()` should be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({\n",
    "    'body/block/layout': C('layout'),\n",
    "    'common/conv/use_bias': C('bias'),\n",
    "})\n",
    "\n",
    "\n",
    "# For reference: previous train_template definition \n",
    "#     train_template = (Pipeline()\n",
    "#                 .init_variable('loss', init_on_each_run=list)\n",
    "#                 .init_model('dynamic', VGG7, 'conv', config=model_config) # Note model class defined explicitly\n",
    "#                 .to_array()\n",
    "#                 .train_model('conv', \n",
    "#                              images=B('images'), labels=B('labels'),\n",
    "#                              fetches='loss', save_to=V('loss', mode='w'))\n",
    "#     )\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('loss', init_on_each_run=list)\n",
    "            .init_model('dynamic', C('model'), 'conv', config=model_config) # Model class defined via named expression\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new research as before but also add the grid of parameters with `add_grid` method. After that we run the research, and it takes much longer because we are now running 5 different experiments 2 times each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=2, n_iters=ITERATIONS, name='vgg_layout_bias_research', bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research results now contain new columns *layout*, *bias* and *model* with corresponding parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling *load_results* output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine different config options in a single-column string representation we can pass `use_alias=True` to `load_results` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results(use_alias=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful when comparing separate experiments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "for i, (config, df) in enumerate(results.groupby('config')):\n",
    "    x, y = i//2, i%2\n",
    "    df.pivot(index='iteration', columns='repetition', values='loss').plot(ax=ax[x, y])\n",
    "    ax[x, y].set_title(config)\n",
    "    ax[x, y].set_xlabel('iteration')\n",
    "    ax[x, y].set_ylabel('loss')\n",
    "    ax[x, y].grid(True)\n",
    "    ax[x, y].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter the results to use certain parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(aliases={'model': 'VGG7'}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does effectively the same but when passing `config` we define actual parameter values (like model class), not their string representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(configs={'model': VGG7}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get results corresponding to certain repetitions of experiments or certain iterations.\n",
    "\n",
    "Here we have only one output variable - *loss* - but if we had many we could also load only some of them using `variables` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(repetitions=1, iterations=[0,9], variables=['loss']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved researches \n",
    "\n",
    "<font color=red size=5>**BROKEN**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, after each run of a research a folder with log information and results is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in os.listdir():\n",
    "    print(\"{}\\t{}\".format('Dir' if os.path.isdir(name) else 'File', name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Research.load` class method to load a previousely run research by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_research = Research.load('vgg_layout_bias_research')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check its parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_research.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run it one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_research.run(n_reps=4, n_iters=ITERATIONS, name='vgg7_research_1_load', bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `n_reps=4, name='vgg7_research_1_load'` from `run` arguments are ignored and the values from the loaded research are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_research.load_results().sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex execution strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would like to run more than a single train pipeline.\n",
    "\n",
    "Let's define a test pipeline to monitor a model's loss on test set while learning. \n",
    "\n",
    "We start with defining a train pipeline as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Option('layout', ['cna', 'can'])\n",
    "\n",
    "model_config={\n",
    "    'inputs/images/shape': (28, 28, 1),\n",
    "    'inputs/labels/classes': 10,\n",
    "    'inputs/labels/name': 'targets',\n",
    "    'initial_block/inputs': 'images',\n",
    "    'body/block/layout': C('layout'),\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('train_loss')\n",
    "            .init_model('dynamic', VGG7, 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('train_loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the test pipeline to be run on the whole test set from time to time during training. \n",
    "\n",
    "In order to get this, we specify lazy-run in test pipeline with `n_epochs=1` and we pass `run=True` to research's `add_pipeline`. These 2 parameter values tell Research to run the pipeline on the whole test set for 1 epoch, instead of running it batch-wise (which is how the test pipeline is run). We also pass `execute=100` parameter to `add_pipeline` to tell Research that this pipeline should be executed only on each 100-th iteration.\n",
    "\n",
    "`execute` parameter defines frequency of pipeline execution. One can tell research to execute a pipeline periodically and pass an `int`, or to execute it on specific iteration by passing zero-based iteration number in following format: `'#{it_no}'` or by simply passing `'last'`. `execute` can also be a list, for example `execute=[\"#0', '#123', 100, 'last']` means that a pipeline will be executed on first iteration, 124-th iteration, every 100-th iteration and the last one.\n",
    "\n",
    "To let the test pipeline to use the model trained by train pipeline we pass this model via `C('import_from')` named expression. We call `import_model` of test pipeline and pass `import_from='train_ppl'` parameter to `add_pipeline`, where `'train_ppl'` is the name of our train pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = mnist.test.p.run_later(BATCH_SIZE, shuffle=True, n_epochs=1) #Note  n_epochs=1\n",
    "test_template = (Pipeline()\n",
    "                 .init_variable('test_loss')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                                images=B('images'), labels=B('labels'),\n",
    "                                fetches='loss', save_to=V('test_loss', mode='w')))\n",
    "\n",
    "test_ppl = test_root + test_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EXECUTE_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, variables='test_loss', name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl') # Note run=True\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_iters=ITERATIONS, name='train_test_research', bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our research results contain entries for test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform some computations while running pipelines, for example to acquire values that are not produced by the model explicitely, one can use functions.\n",
    "\n",
    "We will now add a function to calculate model's accuracy on test set\n",
    "\n",
    "We redefine test pipeline removing `'test_loss'` and adding variables to store predictions and metrics computed for these predictions with `gather_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (Pipeline()\n",
    "                 .init_variable('predictions')\n",
    "                 .init_variable('metrics')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                                images=B('images'), labels=B('labels'),\n",
    "                                fetches='predictions', save_to=V('predictions'))\n",
    "                .gather_metrics('class', targets=B('labels'), predictions=V('predictions'), \n",
    "                                fmt='logits', axis=-1, save_to=V('metrics')))\n",
    "\n",
    "test_ppl = test_root + test_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to be executed during experiment's iterations. Such functions take 2 required parameters: `iteration` and `experiment` which are fed to it by Research and optional keyword parameters which should be provided externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(iteration, experiment, pipeline):\n",
    "    pipeline = experiment[pipeline].pipeline\n",
    "    metrics = pipeline.get_variable('metrics')\n",
    "    return metrics.evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new reseach. Note that we now don't ask the test pipeline to write any variables to results explicitely (we don't pass `variables` parameter to corresponding `add_pipeline`.\n",
    "\n",
    "To add a function we call `add_function` method, providing it with a function object and`returns` parameters that specifies what should be written to research results.  We also pass keyword parameter `pipeline='test_ppl'` that will be substituted to `get_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .add_function(get_accuracy, returns='accuracy', name='test_accuracy_fn', \n",
    "                          execute=TEST_EXECUTE_FREQ, pipeline='test_ppl')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=1, n_iters=ITERATIONS, name='fun_research', bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research results now contain accuracy from `'test_accuracy_fn'`\n",
    "\n",
    "We can load results that come from a certain source by passing `names` parameter to `load_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(names='test_accuracy_fn').sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually collecting metrics is a very common case so there is a special method `get_metrics`. We pass pipeline name to collect metrics from (`pipeline`), the name of the named expression which stores collected metrics (`metrics_var`) and what to calculate(`metrics_name`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .get_metrics(pipeline='test_ppl', metrics_var='metrics', metrics_name='accuracy',\n",
    "                         returns='accuracy', execute=TEST_EXECUTE_FREQ)\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=1, n_iters=ITERATIONS, name='get_metrics_research', bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_metrics` implicitely adds a function with a name *\\[pipeline_name\\]_metrics* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(names='test_ppl_metrics').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
