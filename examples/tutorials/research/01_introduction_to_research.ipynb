{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Research Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces Research functionality of batchflow.\n",
    "\n",
    "Research class allows easily experimenting with models parameters and entire test and train workflow configurations as well as saving and loading results of experiments in a unified form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some useful imports and constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from batchflow import Pipeline, B, C, V, D\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow.models.tf import VGG7, VGG16\n",
    "from batchflow.research import Research, Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "ITERATIONS=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_previous_results(res_name):\n",
    "    if os.path.exists(res_name):\n",
    "        shutil.rmtree(res_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-experiment Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple pipeline that loads some MNIST data, and trains a VGG7 model on it. It also saves the loss on each iteration in a pipeline variable. Let's call it an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data and Creating Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call a lazy version of pipeline's `run` method to define batch size to use. We pass `n_epochs=None`, because the duration of our experiment will be controlled by Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()\n",
    "train_root = mnist.train.p.run_later(BATCH_SIZE, shuffle=True, n_epochs=None)\n",
    "\n",
    "model_config={\n",
    "    'inputs/images/shape': B('image_shape'),\n",
    "    'inputs/labels/classes': D('num_classes'),\n",
    "    'inputs/labels/name': 'targets',\n",
    "    'initial_block/inputs': 'images',\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('loss')\n",
    "            .init_model('dynamic', VGG7, 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpliest thing we can do with Research is running this experiment several times to see how loss'es dynamics changes from run to run.\n",
    "\n",
    "To do this we define a Research object and add the pipeline to it. We call `add_pipeline` method and pass the `train_ppl` as the first parameter. The `variables` parameter gets a string or a list of strings that indicate which pipeline variables will be monitored by Research and written to research results on each iteration. We also provide `name` that will be written to results to indicate from where actually these results come.\n",
    "\n",
    "`logging=True` adds additional logging of the pipeline execution to log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train_ppl', logging=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Research "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each research is assigned with a name and writes its results to a folder with this name. The names must be unique, so if one attempts to run a research with a name that already exists, an error will be thrown. In the cell below we clear the results of previous research runs so as to allow multiple runs of a research. This is done solely for purposes of ths tutorial and should not be done in real work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name='vgg7_research'\n",
    "clear_previous_results(res_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run this Research.\n",
    "\n",
    "We call a Research object's `run` method and we pass following parameters:\n",
    "* `n_iters` - how many iterations will the experiment consist of. Each iteration here consists of processing a single batch\n",
    "* `n_reps` - how many times we run our experiment\n",
    "* `bar` - to provide a tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research vgg7_research is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 4 jobs with 1000 iterations. Totally: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 547/4000 [00:42<04:27, 12.89it/s] "
     ]
    }
   ],
   "source": [
    "research.run(n_reps=4, n_iters=ITERATIONS, name=res_name, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Research Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each research is assigned with a `name` argument provided to `run`.\n",
    "Results of the research and its log are saved in a folder with the same name in the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the experiments' results we call `load_result` method which returns a Pandas Dataframe. We can see that it has 4 collumns which contain \n",
    "* repetition - The number of experiment run\n",
    "* iteration - the number of iteration\n",
    "* name - the source of the variable\n",
    "* loss - this is the variable name that we provided to `add_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now draw a nice plot showing our loss dynamics on each experiment repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pivot(index='iteration', columns='repetition', values='loss').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned to run multiple repetitions of a single experiment with Research. \n",
    "\n",
    "We can also run several experiments with different parameters in one research. Suppose we want to compare the performance of VGG7 and VGG16 models with different layouts ('convolution-normalization-activation' vs 'convolution-activation-normalization') with default bias and pool_strides settings (which are no bias and 2) and we would also like to compare VGG7 with pool_stride 1 and VGG16 with pool_stride 2, both with bias and default 'cna' layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a grid of parameters as follows. \n",
    "We define an Option that consists of the parameter to vary and a list of values that we want to try in our research. Each parameter value defines a node in a parameter grid. We can add grids to unite the nodes, multiply grids to get Cartesian product and multiply options node-wise with `Option.product()`.\n",
    "\n",
    "`grid.gen_configs()` returns a generator that yields one node (that is, a single experiment specification) at a time. Printing a list of all nodes shows us all experiment modifications in a dict-like mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = (Option('layout', ['cna', 'can']) * Option('model', [VGG7, VGG16]) * Option('bias', [False]) * Option('stride', [2])\n",
    "        +  Option('layout', ['cna']) * Option('bias', [True]) * Option.product(Option('model', [VGG7, VGG16]), Option('stride', [1, 2])))\n",
    "list(grid.gen_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pipelines With Variable Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now update `model_config` so that it could read the values from the grid.\n",
    "\n",
    "We pass named expressions as parameters values with names from our parameter grid. \n",
    "We define layout, pool_strides and bias in the model config and we pass model type as a named expression to `init_model` method of the pipeline (config option named expression `C()` should be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({\n",
    "    'body/block/layout': C('layout'),\n",
    "    'body/block/pool_strides': C('stride'),\n",
    "    'common/conv/use_bias': C('bias'),\n",
    "})\n",
    "\n",
    "\n",
    "# For reference: previous train_template definition \n",
    "#     train_template = (Pipeline()\n",
    "#                 .init_variable('loss', init_on_each_run=list)\n",
    "#                 .init_model('dynamic', VGG7, 'conv', config=model_config) # Note model class defined explicitly\n",
    "#                 .to_array()\n",
    "#                 .train_model('conv', \n",
    "#                              images=B('images'), labels=B('labels'),\n",
    "#                              fetches='loss', save_to=V('loss', mode='w'))\n",
    "#     )\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('loss', init_on_each_run=list)\n",
    "            .init_model('dynamic', C('model'), 'conv', config=model_config) # Model class defined via named expression\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Grid To Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new research as before but also add the grid of parameters with `add_grid` method. After that we run the research, and it takes much longer because we are now running 6 different experiments 2 times each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name = 'vgg_layout_bias_poolstrides_research'\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=2, n_iters=ITERATIONS, name=res_name, bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research results now contain new columns *layout*, *stride*, *bias* and *model* with corresponding parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling *load_results* Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Results With Single-column Config Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine different config options in a single-column string representation we can pass `use_alias=True` to `load_results` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results(use_alias=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful when comparing separate experiments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "for i, (config, df) in enumerate(results.groupby('config')):\n",
    "    x, y = i//2, i%2\n",
    "    df.pivot(index='iteration', columns='repetition', values='loss').plot(ax=ax[x, y])\n",
    "    ax[x, y].set_title(config)\n",
    "    ax[x, y].set_xlabel('iteration')\n",
    "    ax[x, y].set_ylabel('loss')\n",
    "    ax[x, y].grid(True)\n",
    "    ax[x, y].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Results With Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter the results to use certain parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(aliases={'model': 'VGG7'}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does effectively the same but when passing `config` we define actual parameter values (like model class), not their string representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(configs={'model': VGG7}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get results corresponding to certain repetitions of experiments or certain iterations.\n",
    "\n",
    "Here we have only one output variable - *loss* - but if we had many we could also load only some of them using `variables` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(repetitions=1, iterations=[0,9], variables=['loss']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, after each run of a research a folder with log information and results is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Research.load` class method to load a previousely run research by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_research = Research.load('vgg_layout_bias_poolstrides_research')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check its parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_research.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run it one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name = 'vgg_layout_bias_poolstrides_research_loaded'\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "loaded_research.run(n_reps=4, n_iters=ITERATIONS, name=res_name, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `n_reps=4` from `run` arguments is ignored and the values from the loaded research are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_research.load_results().sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Execution Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would like to run more than a single train pipeline.\n",
    "\n",
    "Let's define a test pipeline to monitor a model's loss on test set while learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Train Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a train pipeline as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Option('layout', ['cna', 'can'])\n",
    "\n",
    "model_config={\n",
    "    'inputs/images/shape': B('image_shape'),\n",
    "    'inputs/labels/classes': D('num_classes'),\n",
    "    'inputs/labels/name': 'targets',\n",
    "    'initial_block/inputs': 'images',\n",
    "    'body/block/layout': C('layout'),\n",
    "}\n",
    "\n",
    "train_template = (Pipeline()\n",
    "            .init_variable('train_loss')\n",
    "            .init_model('dynamic', VGG7, 'conv', config=model_config)\n",
    "            .to_array()\n",
    "            .train_model('conv', \n",
    "                         images=B('images'), labels=B('labels'),\n",
    "                         fetches='loss', save_to=V('train_loss', mode='w'))\n",
    ")\n",
    "\n",
    "train_ppl = train_root + train_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Test Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the test pipeline to be run on the whole test set from time to time during training. \n",
    "\n",
    "In order to get this, we specify lazy-run in test pipeline with `n_epochs=1` and we pass `run=True` to research's `add_pipeline`. These 2 parameter values tell Research to run the pipeline on the whole test set for 1 epoch, instead of running it batch-wise (which is how the test pipeline is run). We will also pass `execute=100` parameter to `add_pipeline` to tell Research that this pipeline should be executed only on each 100-th iteration.\n",
    "\n",
    "To let the test pipeline to use the model trained by train pipeline we pass this model via `C('import_from')` named expression. We will set its value when creating `Research`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = mnist.test.p.run_later(BATCH_SIZE, shuffle=True, n_epochs=1) #Note  n_epochs=1\n",
    "test_template = (Pipeline()\n",
    "                 .init_variable('test_loss')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                                images=B('images'), labels=B('labels'),\n",
    "                                fetches='loss', save_to=V('test_loss', mode='w')))\n",
    "\n",
    "test_ppl = test_root + test_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Research with 2 Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we pass `execute=100` parameter to `add_pipeline` to tell Research that this pipeline should be executed only on each 100-th iteration.\n",
    "\n",
    "`execute` parameter defines frequency of pipeline execution. One can tell research to execute a pipeline periodically and pass an `int`, or to execute it on specific iteration by passing zero-based iteration number in following format: `'#{it_no}'` or by simply passing `'last'`. `execute` can also be a list, for example `execute=[\"#0', '#123', 100, 'last']` means that a pipeline will be executed on first iteration, 124-th iteration, every 100-th iteration and the last one.\n",
    "\n",
    "In order to import model from train to test pipeline, we pass `import_from='train_ppl'` parameter to `add_pipeline`, where `'train_ppl'` is the name of our train pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EXECUTE_FREQ = 100\n",
    "\n",
    "res_name = 'train_test_research'\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, variables='test_loss', name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl') # Note run=True\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_iters=ITERATIONS, name=res_name, bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our research results contain entries for test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(names='test_ppl').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform some computations while running pipelines, for example to acquire values that are not produced by the model explicitely, one can use functions.\n",
    "\n",
    "We will now add a function to calculate model's accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Test Pipeline That Gathers Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine test pipeline removing `'test_loss'` and adding variables to store predictions and metrics computed for these predictions with `gather_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (Pipeline()\n",
    "                 .init_variable('predictions')\n",
    "                 .init_variable('metrics')\n",
    "                 .import_model('conv', C('import_from'))\n",
    "                 .to_array()\n",
    "                 .predict_model('conv', \n",
    "                                images=B('images'), labels=B('labels'),\n",
    "                                fetches='predictions', save_to=V('predictions'))\n",
    "                 .gather_metrics('class', targets=B('labels'), predictions=V('predictions'), \n",
    "                                fmt='logits', axis=-1, save_to=V('metrics')))\n",
    "\n",
    "test_ppl = test_root + test_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions To Be Executed By Reasearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to be executed during experiment's iterations. Such functions take 2 required parameters: `iteration` and `experiment` which are fed to it by Research and optional keyword parameters which should be provided externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(iteration, experiment, pipeline):\n",
    "    pipeline = experiment[pipeline].pipeline\n",
    "    metrics = pipeline.get_variable('metrics')\n",
    "    return metrics.evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Functions To Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new reseach. Note that we now don't ask the test pipeline to write any variables to results explicitely (we don't pass `variables` parameter to corresponding `add_pipeline`.\n",
    "\n",
    "To add a function we call `add_function` method, providing it with a function object and`returns` parameters that specifies what should be written to research results.  We also pass keyword parameter `pipeline='test_ppl'` that will be substituted to `get_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name = 'fun_research'\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .add_function(get_accuracy, returns='accuracy', name='test_accuracy_fn', \n",
    "                          execute=TEST_EXECUTE_FREQ, pipeline='test_ppl')\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=1, n_iters=ITERATIONS, name=res_name, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research results now contain accuracy from `'test_accuracy_fn'`\n",
    "\n",
    "We can load results that come from a certain source by passing `names` parameter to `load_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(names='test_accuracy_fn').sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually collecting metrics is a very common case so there is a special method `get_metrics`. We pass pipeline name to collect metrics from (`pipeline`), the name of the named expression which stores collected metrics (`metrics_var`) and what to calculate(`metrics_name`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name = 'get_metrics_research'\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_ppl, variables='train_loss', name='train_ppl')\n",
    "            .add_pipeline(test_ppl, name='test_ppl',\n",
    "                         execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .get_metrics(pipeline='test_ppl', metrics_var='metrics', metrics_name='accuracy',\n",
    "                         returns='accuracy', execute=TEST_EXECUTE_FREQ)\n",
    "            .add_grid(grid))\n",
    "\n",
    "research.run(n_reps=1, n_iters=ITERATIONS, name=res_name, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_metrics` implicitely adds a function with a name *\\[pipeline_name\\]_metrics* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results(names='test_ppl_metrics').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
