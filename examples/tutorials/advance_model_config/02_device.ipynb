{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the point of multi-GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need more than one GPU if model training time consumes a significant fraction of execution pipeline time. \n",
    "Therefore, if you have several GPUs, you can use all of them to train model. This will speed up training process of the model. \n",
    "\n",
    "Parameter `device` allows train model on multiple GPU (Сreates a copy of model on each selected GPU).\n",
    "Initialization of large model on a large number of GPU may take some time (minuts or tens of minutes)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from batchflow import Pipeline, B, C, V, D\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which GPU(s) to be used. More about it in [CUDA documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=5,6\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=5,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset, define a pipeline config, define a default model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(bar=True)\n",
    "\n",
    "config = dict(model=ResNet18)\n",
    "\n",
    "model_config = {'inputs/images/shape': B.image_shape,\n",
    "                'inputs/labels/classes': D.num_classes,\n",
    "                'initial_block/inputs': 'images'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `device` and set up 2 GPUs to train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({'device': ['GPU:0', 'GPU:1']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `device` can be string, list of strings or regular expression.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "'device': 'GPU:0'                     # Used only GPU:0\n",
    "'device': ['GPU:0', 'GPU:1', 'GPU:2'] # Used GPU:0, GPU:1 and GPU:2\n",
    "'device': 'GPU:*'                     # Used all avalible GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **BATCH SIZE MUST BE DIVISIBLE BY NUMBER OF DEVICES WITHOUT A REMAINDER! (IF MICROBATHING ~~BATCH SIZE~~ MICROBATCH SIZE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on several GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<batchflow.once_pipeline.OncePipeline at 0x7f2640e2db38>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_multi = (Pipeline(config=config)\n",
    "                        .to_array()\n",
    "                        .train_model('conv_nn', fetches='loss', \n",
    "                                     images=B.images, labels=B.labels,\n",
    "                                     save_to=V('loss_history', mode='a'), use_lock=True))\n",
    "\n",
    "(template_multi.before\n",
    " .init_variable('loss_history', [])\n",
    " .init_model('dynamic', C('model'),'conv_nn',\n",
    "             config=model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [01:03<00:00, 24.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f265482b128>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_multi = template_multi << dataset.train\n",
    "pipeline_multi.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training lasted 1:03 on two GPUs. If we add more GPUs, we get even less training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU and microbathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Schematic illustration of the formation of batches to each GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Batch_microbatch_GPU.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `microbatch` and `device` at the same time. If we have huge batches it be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add microbathing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({'microbatch': 32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with several GPUs and microbatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<batchflow.once_pipeline.OncePipeline at 0x7f23ac624e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_multi_micro = (Pipeline(config=config)\n",
    "                        .to_array()\n",
    "                        .train_model('conv_nn', fetches='loss', \n",
    "                                     images=B.images, labels=B.labels,\n",
    "                                     save_to=V('loss_history', mode='a'), \n",
    "                                     use_lock=True))\n",
    "\n",
    "(template_multi_micro.before\n",
    " .init_variable('loss_history', [])\n",
    " .init_model('dynamic', C('model'),'conv_nn',\n",
    "             config=model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [01:28<00:00, 14.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f23ac624828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_multi_micro = template_multi_micro << dataset.train\n",
    "pipeline_multi_micro.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training finish without error it means that we can use `device` and `microbatch` together. \n",
    "Look at training time of current model, the previous model, the model from [03_ready_to_use_model_tf](../03_ready_to_use_model_tf.ipynb) and the model \n",
    "from [01_microbatch](./01_microbatch.ipynb). When we added one more gpu to the model from [03_ready_to_use_model_tf](../03_ready_to_use_model_tf.ipynb) (made previous model) we got significant speed up, training time was reduced by 2 times! \n",
    "And when we added gpu to the model from [01_microbatch](./01_microbatch.ipynb) (made current model) we also got speed up.\n",
    "\n",
    "If you will use several GPUs, you can save tens of hours training the model or even more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next tutorial about [different training procedures](./03_train_steps.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
