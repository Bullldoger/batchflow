{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the point of microbathing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, bigger batch size allows for better gradients estimates, thus helping training models. Unfortunately, there is a hard constraint on device memory: even the modern accelerators can't fit more that hundreds of gigabytes, which is sometimes just not enough. This is where `microbatch` comes to the rescue: it allows to evenly split batch data into multiple pieces (called microbatches), evaluate gradients on each of them separately, and the apply average value of gradient directly to the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from batchflow import Pipeline, B, C, V, D\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which GPU(s) to be used. More about it in [CUDA documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset, define a pipeline config, define a default model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(bar=True)\n",
    "\n",
    "config = dict(model=ResNet18)\n",
    "\n",
    "model_config = {'inputs/images/shape': B.image_shape,\n",
    "                'inputs/labels/classes': D.num_classes,\n",
    "                'initial_block/inputs': 'images'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add `microbatch` to model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({'microbatch': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we run the pipeline, the model will receive batches with size 16 not BATCH_SIZE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **MICROBATH SIZE MUST BE A DIVISOR OF THE BATCH SIZE!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<batchflow.once_pipeline.OncePipeline at 0x7fefd02bd390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_template_microbatch = (Pipeline(config=config)\n",
    "                  .to_array()\n",
    "                  .train_model('conv_nn', fetches='loss', \n",
    "                               images=B.images, labels=B.labels,\n",
    "                               save_to=V('loss_history', mode='a')))\n",
    "\n",
    "(train_template_microbatch.before\n",
    " .init_variable('loss_history', [])\n",
    " .init_model('dynamic', C('model'), 'conv_nn', config=model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [01:53<00:00,  9.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7ff077be4ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline_microbatch = train_template_microbatch << dataset.train\n",
    "train_pipeline_microbatch.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn’t have `microbatch` in the model configuration and we want to split batch into microbatches. We could run pipeline with parameter `microbatch=microbatch_size`:\n",
    "```python\n",
    "train_pipeline_microbatch.run(BATCH_SIZE, shuffle=True, n_epochs=1, microbatch=16, bar=True, drop_last=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to pipeline from [03_ready_to_use_model_tf](../03_ready_to_use_model_tf.ipynb) we can see, that `microbatch` allows us to leverage simple trade-off between bigger batch size (thus model performance) and model training time. **If the data in batch can fit memory constraints, there is no reason to use `microbatch` due to inherently slower processing of batches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train models using `microbatch` and you might want to see next tutorial about [multiple devices](./02_device.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
