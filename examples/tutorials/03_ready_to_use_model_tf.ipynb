{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ready to use TensorFlow model with a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the following line is not required if BatchFlow is installed as a python package.\n",
    "sys.path.append(\"../..\")\n",
    "from batchflow import Pipeline, B, C, F, V\n",
    "from batchflow.opensets import MNIST, CIFAR10\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you comment out the line below, the training will take much more time and the accuracy might slightly decrease.\n",
    "So it is always a good idea to import [best_practice](https://analysiscenter.github.io/batchflow/intro/best_practice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchflow import best_practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE might be increased for modern GPUs with lots of memory (4GB and higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits frequently used as a baseline for machine learning tasks.\n",
    "\n",
    "Downloading MNIST database might take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading Extractinghttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      " Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "/tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also predefined CIFAR10 and CIFAR100 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a pipeline config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config allows to create flexible pipelines which take parameters.\n",
    "\n",
    "For instance, if you put a model type into config, you can run a pipeline against different models.\n",
    "\n",
    "See [a list of available models](https://analysiscenter.github.io/batchflow/intro/tf_models.html#ready-to-use-models) to choose the one which fits you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(model=ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a template pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A template pipeline is not linked to any dataset. It's just an abstract sequence of actions, so it cannot be executed, but it serves as a convenient building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (Pipeline(config=config)\n",
    "                .init_variable('loss_history', init_on_each_run=list)\n",
    "                .init_variable('current_loss')\n",
    "                .init_model('dynamic', C('model'), 'conv_nn',\n",
    "                            config={'inputs': dict(images={'shape': B('image_shape')},\n",
    "                                                   labels={'classes': 10}),\n",
    "                                    'initial_block/inputs': 'images'})\n",
    "                .to_array()\n",
    "                .train_model('conv_nn', fetches='loss', images=B('images'), labels=B('labels'),\n",
    "                             save_to=V('current_loss'))\n",
    "                .update_variable('loss_history', V('current_loss'), mode='a')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a dataset to a template pipeline to create a runnable pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (train_template << dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline (it might take from a few minutes to a few hours depending on your hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/937 [00:42<22:30,  1.48s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f46b1ed59885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/batchflow/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, init_vars, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pipeline will never stop as n_epochs=None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/batchflow/pipeline.py\u001b[0m in \u001b[0;36m_gen_batch\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0mbatch_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_res\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_pipeline.run(BATCH_SIZE, shuffle=True, n_epochs=1, drop_last=True, bar=True, prefetch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the progress bar often increments by 2 at a time - that's prefetch in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not give much here, though, since almost all time is spent in model training which is performed under a thread-lock one batch after another without any parallelism (otherwise the model would not learn anything as different batches would rewrite one another's model weights updates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much faster than training, but if you don't have GPU it would take some patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 2/156 [00:00<01:08,  2.25it/s]\u001b[A\n",
      "  3%|▎         | 4/156 [00:01<00:50,  2.98it/s]\u001b[A\n",
      "  4%|▍         | 6/156 [00:01<00:38,  3.86it/s]\u001b[A\n",
      "  5%|▌         | 8/156 [00:01<00:31,  4.77it/s]\u001b[A\n",
      "  6%|▌         | 9/156 [00:01<00:26,  5.49it/s]\u001b[A\n",
      "  6%|▋         | 10/156 [00:01<00:24,  6.06it/s]\u001b[A\n",
      "  7%|▋         | 11/156 [00:01<00:21,  6.63it/s]\u001b[A\n",
      "  8%|▊         | 12/156 [00:01<00:20,  7.12it/s]\u001b[A\n",
      "  8%|▊         | 13/156 [00:01<00:19,  7.49it/s]\u001b[A\n",
      "  9%|▉         | 14/156 [00:02<00:18,  7.77it/s]\u001b[A\n",
      " 10%|█         | 16/156 [00:02<00:15,  8.79it/s]\u001b[A\n",
      " 12%|█▏        | 18/156 [00:02<00:14,  9.67it/s]\u001b[A\n",
      " 13%|█▎        | 20/156 [00:02<00:13, 10.36it/s]\u001b[A\n",
      " 14%|█▍        | 22/156 [00:02<00:12, 10.95it/s]\u001b[A\n",
      " 15%|█▌        | 24/156 [00:02<00:11, 11.19it/s]\u001b[A\n",
      " 17%|█▋        | 26/156 [00:03<00:11, 11.47it/s]\u001b[A\n",
      " 18%|█▊        | 28/156 [00:03<00:10, 11.68it/s]\u001b[A\n",
      " 19%|█▉        | 30/156 [00:03<00:10, 11.92it/s]\u001b[A\n",
      " 21%|██        | 32/156 [00:03<00:11, 11.12it/s]\u001b[A\n",
      " 22%|██▏       | 34/156 [00:03<00:11, 10.23it/s]\u001b[A\n",
      " 23%|██▎       | 36/156 [00:04<00:11, 10.59it/s]\u001b[A\n",
      " 24%|██▍       | 38/156 [00:04<00:11, 10.10it/s]\u001b[A\n",
      " 26%|██▌       | 40/156 [00:04<00:11, 10.32it/s]\u001b[A\n",
      " 27%|██▋       | 42/156 [00:04<00:10, 10.87it/s]\u001b[A\n",
      " 28%|██▊       | 44/156 [00:04<00:09, 11.26it/s]\u001b[A\n",
      " 29%|██▉       | 46/156 [00:04<00:09, 11.47it/s]\u001b[A\n",
      " 31%|███       | 48/156 [00:05<00:09, 11.60it/s]\u001b[A\n",
      " 32%|███▏      | 50/156 [00:05<00:08, 11.83it/s]\u001b[A\n",
      " 33%|███▎      | 52/156 [00:05<00:08, 11.92it/s]\u001b[A\n",
      " 35%|███▍      | 54/156 [00:05<00:08, 12.06it/s]\u001b[A\n",
      " 36%|███▌      | 56/156 [00:05<00:08, 12.02it/s]\u001b[A\n",
      " 37%|███▋      | 58/156 [00:05<00:08, 12.13it/s]\u001b[A\n",
      " 38%|███▊      | 60/156 [00:06<00:07, 12.19it/s]\u001b[A\n",
      " 40%|███▉      | 62/156 [00:06<00:07, 12.20it/s]\u001b[A\n",
      " 41%|████      | 64/156 [00:06<00:07, 12.20it/s]\u001b[A\n",
      " 42%|████▏     | 66/156 [00:06<00:07, 12.28it/s]\u001b[A\n",
      " 44%|████▎     | 68/156 [00:06<00:07, 12.10it/s]\u001b[A\n",
      " 45%|████▍     | 70/156 [00:06<00:07, 12.01it/s]\u001b[A\n",
      " 46%|████▌     | 72/156 [00:07<00:06, 12.04it/s]\u001b[A\n",
      " 47%|████▋     | 74/156 [00:07<00:06, 12.09it/s]\u001b[A\n",
      " 49%|████▊     | 76/156 [00:07<00:06, 12.19it/s]\u001b[A\n",
      " 50%|█████     | 78/156 [00:07<00:06, 11.97it/s]\u001b[A\n",
      " 51%|█████▏    | 80/156 [00:07<00:06, 11.62it/s]\u001b[A\n",
      " 53%|█████▎    | 82/156 [00:07<00:06, 11.43it/s]\u001b[A\n",
      " 54%|█████▍    | 84/156 [00:08<00:06, 11.00it/s]\u001b[A\n",
      " 55%|█████▌    | 86/156 [00:08<00:06, 11.26it/s]\u001b[A\n",
      " 56%|█████▋    | 88/156 [00:08<00:05, 11.38it/s]\u001b[A\n",
      " 58%|█████▊    | 90/156 [00:08<00:05, 11.62it/s]\u001b[A\n",
      " 59%|█████▉    | 92/156 [00:08<00:05, 11.61it/s]\u001b[A\n",
      " 60%|██████    | 94/156 [00:08<00:05, 11.57it/s]\u001b[A\n",
      " 62%|██████▏   | 96/156 [00:09<00:05, 11.58it/s]\u001b[A\n",
      " 63%|██████▎   | 98/156 [00:09<00:04, 11.76it/s]\u001b[A\n",
      " 64%|██████▍   | 100/156 [00:09<00:04, 11.83it/s]\u001b[A\n",
      " 65%|██████▌   | 102/156 [00:09<00:04, 11.88it/s]\u001b[A\n",
      " 67%|██████▋   | 104/156 [00:09<00:04, 11.97it/s]\u001b[A\n",
      " 68%|██████▊   | 106/156 [00:09<00:04, 12.09it/s]\u001b[A\n",
      " 69%|██████▉   | 108/156 [00:10<00:03, 12.09it/s]\u001b[A\n",
      " 71%|███████   | 110/156 [00:10<00:03, 12.08it/s]\u001b[A\n",
      " 72%|███████▏  | 112/156 [00:10<00:03, 12.11it/s]\u001b[A\n",
      " 73%|███████▎  | 114/156 [00:10<00:03, 12.13it/s]\u001b[A\n",
      " 74%|███████▍  | 116/156 [00:10<00:03, 11.97it/s]\u001b[A\n",
      " 76%|███████▌  | 118/156 [00:10<00:03, 12.00it/s]\u001b[A\n",
      " 77%|███████▋  | 120/156 [00:11<00:03, 11.96it/s]\u001b[A\n",
      " 78%|███████▊  | 122/156 [00:11<00:02, 11.99it/s]\u001b[A\n",
      " 79%|███████▉  | 124/156 [00:11<00:02, 12.02it/s]\u001b[A\n",
      " 81%|████████  | 126/156 [00:11<00:02, 12.05it/s]\u001b[A\n",
      " 82%|████████▏ | 128/156 [00:11<00:02, 12.01it/s]\u001b[A\n",
      " 83%|████████▎ | 130/156 [00:11<00:02, 11.99it/s]\u001b[A\n",
      " 85%|████████▍ | 132/156 [00:12<00:01, 12.00it/s]\u001b[A\n",
      " 86%|████████▌ | 134/156 [00:12<00:01, 12.05it/s]\u001b[A\n",
      " 87%|████████▋ | 136/156 [00:12<00:01, 12.07it/s]\u001b[A\n",
      " 88%|████████▊ | 138/156 [00:12<00:01, 10.85it/s]\u001b[A\n",
      " 90%|████████▉ | 140/156 [00:12<00:01, 10.32it/s]\u001b[A\n",
      " 91%|█████████ | 142/156 [00:13<00:01, 10.72it/s]\u001b[A\n",
      " 92%|█████████▏| 144/156 [00:13<00:01, 10.98it/s]\u001b[A\n",
      " 94%|█████████▎| 146/156 [00:13<00:00, 11.28it/s]\u001b[A\n",
      " 95%|█████████▍| 148/156 [00:13<00:00, 11.26it/s]\u001b[A\n",
      " 96%|█████████▌| 150/156 [00:13<00:00, 11.04it/s]\u001b[A\n",
      " 97%|█████████▋| 152/156 [00:13<00:00, 10.93it/s]\u001b[A\n",
      " 99%|█████████▊| 154/156 [00:14<00:00, 10.76it/s]\u001b[A\n",
      "100%|██████████| 156/156 [00:14<00:00, 10.96it/s]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "test_pipeline = (dataset.test.p\n",
    "                .import_model('conv_nn', train_pipeline)\n",
    "                .init_variable('predictions') \n",
    "                .init_variable('metrics', init_on_each_run=None) \n",
    "                .to_array()\n",
    "                .predict_model('conv_nn', fetches='predictions', images=B('images'), labels=B('labels'),\n",
    "                               save_to=V('predictions'))\n",
    "                .gather_metrics('class', targets=B('labels'), predictions=V('predictions'),\n",
    "                                fmt='logits', axis=-1, save_to=V('metrics'), mode='w')\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=1, drop_last=True, bar=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the accumulated [metrics information](https://analysiscenter.github.io/batchflow/intro/models.html#model-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = test_pipeline.get_variable('metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easiliy calculate any metrics we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.890625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'false_positive_rate': array([0.01785714, 0.        , 0.        , 0.0483871 , 0.        ,\n",
       "        0.        , 0.        , 0.03703704, 0.01694915, 0.        ]),\n",
       " 'false_negative_rate': array([0.        , 0.125     , 0.33333333, 0.        , 0.        ,\n",
       "        0.14285714, 0.        , 0.2       , 0.        , 1.        ])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.evaluate(['false_positive_rate', 'false_negative_rate'], multiclass=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "After learning the model, you may need to save it. It's easy to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.save_model('conv_nn', path='path/to/save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the image augmentation tutorial](./06_image_augmentation.ipynb) or return to the [table of contents](./00_description.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
