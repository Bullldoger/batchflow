{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ready to use model with a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the following line is not required if Dataset is installed as a python package.\n",
    "sys.path.append(\"../..\")\n",
    "from dataset import Pipeline, B, C, F, V\n",
    "from dataset.opensets import MNIST, CIFAR10\n",
    "from dataset.models.tf import ResNet18\n",
    "from dataset.models.metrics import ClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you comment out the line below, the training will take much more time and the accuracy might slightly decrease.\n",
    "So it is always a good idea to import [best_practice](https://analysiscenter.github.io/dataset/intro/best_practice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import best_practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE might be increased for modern GPUs with lots of memory (4GB and higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits frequently used as a baseline for machine learning tasks.\n",
    "\n",
    "Downloading MNIST database might take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also predefined CIFAR10 and CIFAR100 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a pipeline config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config allows to create flexible pipelines which take parameters.\n",
    "\n",
    "For instance, if you put a model type into config, you can run a pipeline against different models.\n",
    "\n",
    "See [a list of available models](https://analysiscenter.github.io/dataset/intro/tf_models.html#ready-to-use-models) to choose the one which fits you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(model=ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a template pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A template pipeline is not linked to any dataset. It's just an abstract sequence of actions, so it cannot be executed, but it serves as a convenient building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (Pipeline(config=config)\n",
    "                .init_variable('loss_history', init_on_each_run=list)\n",
    "                .init_variable('current_loss')\n",
    "                .init_model('dynamic', C('model'), 'conv_nn',\n",
    "                            config={'inputs': dict(images={'shape': B('image_shape')},\n",
    "                                                   labels={'classes': 10, 'transform': 'ohe', 'name': 'targets'}),\n",
    "                                    'input_block/inputs': 'images'})\n",
    "                .to_array()\n",
    "                .train_model('conv_nn', fetches='loss',\n",
    "                                     feed_dict={'images': B('images'),\n",
    "                                                'labels': B('labels')},\n",
    "                             save_to=V('current_loss'))\n",
    "                .update_variable('loss_history', V('current_loss'), mode='a')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a dataset to a template pipeline to create a runnable pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (train_template << dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline (it might take from a few minutes to a few hours depending on your hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [08:01<00:00,  1.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataset.pipeline.Pipeline at 0x7f9c96a7eef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline.run(BATCH_SIZE, shuffle=True, n_epochs=1, drop_last=True, bar=True, prefetch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the progress bar often increments by 2 at a time - that's prefetch in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not give much here, though, since almost all time is spent in model training which is performed under a thread-lock one batch after another without any parallelism (otherwise the model would not learn anything as different batches would rewrite one another's model weights updates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much faster than training, but if you don't have GPU it would take some patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:28<00:00,  6.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = (dataset.test.p\n",
    "                .import_model('conv_nn', train_pipeline)\n",
    "                .init_variable('predictions') \n",
    "                .init_variable('metrics', init_on_each_run=None) \n",
    "                .to_array()\n",
    "                .predict_model('conv_nn', fetches='predictions',\n",
    "                               feed_dict={'images': B('images'), 'labels': B('labels')},\n",
    "                               save_to=V('predictions'))\n",
    "                .gather_metrics(ClassificationMetrics, targets=B('labels'), predictions=V('predictions'),\n",
    "                                fmt='logits', axis=-1, save_to=V('metrics'), mode='a')\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=1, drop_last=True, bar=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the accumulated [metrics information](https://analysiscenter.github.io/dataset/intro/models.html#model-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = test_pipeline.get_variable('metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easiliy calculate any metrics we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98026842948717952"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'false_negative_rate': array([ 0.00510204,  0.00706714,  0.00290698,  0.00793651,  0.01022495,\n",
       "         0.02693603,  0.02713987,  0.01945525,  0.04620123,  0.04785643]),\n",
       " 'false_positive_rate': array([ 0.00366504,  0.00056484,  0.00558534,  0.00200535,  0.00233178,\n",
       "         0.00109975,  0.00044316,  0.00390799,  0.00144284,  0.00089077])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.evaluate(['false_positive_rate', 'false_negative_rate'], multiclass=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "After learning the model, you may need to save it. It's easy to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.save_model('conv_nn', path='path/to/save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the image augmentation tutorial](./06_image_augmentation.ipynb) or return to the [table of contents](./00_description.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
