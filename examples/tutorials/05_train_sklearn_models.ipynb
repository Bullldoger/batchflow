{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Models With Simple Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) and [Huber classifier](https://en.wikipedia.org/wiki/Huber_loss#Variant_for_classification) from sklearn library using the same pipeline and compare their perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from batchflow.models import SklearnModel\n",
    "from batchflow.opensets import MNIST\n",
    "from batchflow import B, C, V, D, Pipeline\n",
    "from batchflow.utils import plot_images\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the pipeline configs for each of the models.    \n",
    "It has to contain the following keys:\n",
    "* `model_name` \n",
    "* `estimator`  that has to be [SGDClassifiers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.predict_proba) instance.\n",
    "* `predictions_name` - the variable name in the pipeline that contains model's predictions\n",
    "* `metrics_name` - the variable name in the pipeline that contains model's metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_config = {'model_name': 'huber_model', \n",
    "                'estimator': SGDClassifier(loss='modified_huber'),\n",
    "                'metrics_name': 'huber_metrics'}\n",
    "\n",
    "log_config = {'model_name': 'log_model', \n",
    "              'estimator': SGDClassifier(loss='log'),\n",
    "              'metrics_name': 'log_metrics'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize both models with the estimators from the corresponding config keys, i.e. `config['estimator']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_huber_model = Pipeline().init_model('dynamic', SklearnModel, 'huber_model', \n",
    "                                 config={'estimator' : C('estimator')})\n",
    "init_log_model = Pipeline().init_model('dynamic', SklearnModel, 'log_model', \n",
    "                                 config={'estimator' : C('estimator')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline template. We exploit the exact same pipeline various times with different estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline preprocessing image actions include:\n",
    "1. Transform images from `PIL.Image` to `np.array`.   \n",
    "2. Reshape images to `2-dimensional` arrays where the number of rows equal to the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sklearn` models with `partial_fit` attribute support batch wise training and can be integrated into pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (dataset.train.p\n",
    "                    .to_array()\n",
    "                    .add_namespace(np)\n",
    "                    .reshape(B('images'), (B('size'), -1), save_to=B('images'))\n",
    "                    .train_model(C('model_name'), B.images, B.labels, \n",
    "                                classes=range(dataset.num_classes))\n",
    "                    .run_later(64, n_iters=5000, drop_last=True, shuffle=True, bar=True)\n",
    "                 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tie the pipeline's configs to the `train_template`    \n",
    "Now we have ready to use training pipelines for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_train_pipeline = (init_huber_model + train_template) << huber_config\n",
    "log_train_pipeline = (init_log_model + train_template) << log_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:15<00:00, 36.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f9d519bb1d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huber_train_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 4236/5000 [01:52<00:20, 37.85it/s]"
     ]
    }
   ],
   "source": [
    "log_train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import trained models from training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_huber_model = Pipeline().import_model('huber_model', huber_train_pipeline)\n",
    "import_log_model = Pipeline().import_model('log_model', log_train_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps of preprocessing images used in test pipelines.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (dataset.test.p\n",
    "                    .to_array()\n",
    "                    .add_namespace(np)\n",
    "                    .reshape(B('images'), (B('size'), -1), save_to=B('images'))\n",
    "                    .predict_model(C('model_name'), B('images'),\n",
    "                                   save_to=B('predictions'), proba=True)              \n",
    "                    .gather_metrics('class', B('labels'), B('predictions'), num_classes=dataset.num_classes,\n",
    "                                     fmt='proba', axis=1, save_to=V(C('metrics_name')))\n",
    "                    .run_later(200, n_epochs=1, drop_last=False, shuffle=True, bar=True)\n",
    "                    .reshape(B('images'), (B('size'), 28, 28), save_to=B('images'))\n",
    "                ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tie the pipeline configs to the test template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_test_pipeline = (import_huber_model + test_template) << huber_config\n",
    "log_test_pipeline = (import_log_model + test_template) << log_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_test_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the accumulated [metrics information](https://analysiscenter.github.io/batchflow/intro/models.html#model-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_metrics = log_test_pipeline.v('log_metrics')\n",
    "huber_metrics = huber_test_pipeline.v('huber_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [log_metrics.evaluate('acc'), huber_metrics.evaluate('acc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber classifier slightly  outperfoms logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('log_reg accuracy - {0} \\nhuber accuracy - {1}'.format(*acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and pass the batch of images through the trained classifiers and look in more details at the predictions of each of them.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_batch = huber_test_pipeline.next_batch(10, shuffle=42)\n",
    "log_batch = log_test_pipeline.next_batch(10, shuffle=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [huber_batch.predictions, log_batch.predictions]\n",
    "plot_images(log_batch.images, log_batch.labels, predictions,\n",
    "                        classes=None, figsize=(16, 12), models_names=['huber', 'logreg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you have learnt how to train and validate `logistic regression` and `huber` classifier from [sklearn](https://scikit-learn.org/stable/index.html) library using [batchflow pipeline](https://analysiscenter.github.io/batchflow/intro/pipeline.html) functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your futher research you can train [SVM](https://en.wikipedia.org/wiki/Support-vector_machine) classifier or try to add more regularization to the models via passing `alpha` and `l1` arrguments to the `SGDClassifiers` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
