{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces you to the modelling features of BatchFlow library via creating a simple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from batchflow.models.tf import TFModel\n",
    "from batchflow.models.torch import TorchModel\n",
    "from batchflow import Dataset, V, F, B, action, Batch\n",
    "from batchflow.models.metrics import ClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a batch class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a specific batch class is not required, though convenient. Besides, here it helps you get an idea of what batch components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatch(Batch):\n",
    "    \"\"\" Batch class for regression models \"\"\"\n",
    "    components = 'features', 'labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the batches will have 2 components: features and labels. Batch components might be thought of as columns in a table, while batch items are rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Firstly, we consider a linear regression that allows solving tasks where targets are continuous variables.\n",
    "\n",
    "For this reason we generate data from uniform or normal distributions, multiply it by normally distributed weights and add normally distributed noise and then try to predict it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(size, dist='unif', shape=13):\n",
    "    \"\"\" Generation of data to fit linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        data length\n",
    "\n",
    "    dist: {'unif', 'norm'}\n",
    "        sample distribution 'unif' or 'norm'. Default is 'unif'\n",
    "    \n",
    "    shape: int\n",
    "        a length of a feature vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        Uniformly or normally distributed array\n",
    "\n",
    "    y: numpy array\n",
    "        array with some random noize\n",
    "    \"\"\"\n",
    "    if dist == 'unif':\n",
    "        x = np.random.uniform(0, 2, size=(size, shape))\n",
    "    elif dist == 'norm':\n",
    "        x = np.random.normal(size=(size, shape))\n",
    "\n",
    "    w = np.random.normal(loc=1., size=(shape, 1))\n",
    "    error = np.random.normal(loc=0., scale=0.1, size=(size, 1))\n",
    "\n",
    "    y = np.dot(x, w) + error\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `x` and `y` are numpy arrays:\n",
    "- `x` is a matrix with __size__ rows and 13 columns\n",
    "- `y` is a vector of __size__ items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "linear_x, linear_y = generate_linear_data(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create a dataset (an instance of Dataset class) which generates batches of __MyBatch__ class. \n",
    "\n",
    "Even though the dataset does not have any data yet, it contains the full index of dataset items. So we can split it into __train__ and __test__ parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dset = Dataset(size, batch_class=MyBatch)\n",
    "linear_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation the dataset is empty, until data is loaded with [a pipeline](https://analysiscenter.github.io/batchflow/intro/pipeline.html) which allows you to use and perform action-methods from the [batch class](https://analysiscenter.github.io/batchflow/intro/batch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (linear_dset.train.p\n",
    "                       .load(src=(linear_x, linear_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline above only loads data. Clearly, it doesn't train a linear regression, therefore it is not enough for us.\n",
    "\n",
    "Hence, we need to create a linear regression model. For more details on how to create your own model see [the documentation](https://analysiscenter.github.io/batchflow/intro/tf_models#how-to-write-a-custom-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRegressionModel(TFModel):\n",
    "    @classmethod\n",
    "    def body(cls, inputs, units, name='body', **kwargs):\n",
    "        with tf.variable_scope(name):\n",
    "            dense = tf.layers.dense(inputs, units=units, name='dense')\n",
    "        return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRegressionModel(TorchModel):\n",
    "    @classmethod\n",
    "    def body(cls, inputs, units, **kwargs):\n",
    "        return nn.Linear(np.prod(inputs.shape[1:]), units) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is ready, you need to train it. The pipeline allows you to do this with just two functions:\n",
    "* [__init_model__](https://analysiscenter.github.io/batchflow/api/batchflow.pipeline.html?highlight=init_model#batchflow.Pipeline.init_model) allows to [configure a model](https://analysiscenter.github.io/batchflow/intro/models.html#adding-a-model-to-a-pipeline)\n",
    "* [__train_model__](https://analysiscenter.github.io/batchflow/api/batchflow.pipeline.html?highlight=train_model#batchflow.Pipeline.train_model) [fits a model with training data](https://analysiscenter.github.io/batchflow/intro/models.html#training-a-model).\n",
    "\n",
    "__init_model__'s config contains __inputs__ sections to configure input tensors (placeholders) parameters:\n",
    "- shape\n",
    "- tensor's name\n",
    "- typical transformations (like one-hot-encoding)\n",
    "and so on.\n",
    "\n",
    "To configure 'inputs' use the __inputs_config__ dict shown in the cell below. This dict has two keys:\n",
    "* __features__ - the name of the placeholder for the input data.\n",
    "* __labels__ - the name of the placeholder for the answers before all transformations.\n",
    "\n",
    "Values for these keys are dicts themselves which describe `features` and `labels` placeholders.\n",
    "\n",
    "For more information see [the documentation](https://analysiscenter.github.io/batchflow/intro/tf_models#how-to-configure-a-model) and [API](https://analysiscenter.github.io/batchflow/api/batchflow.models.tf.base.html#batchflow.models.tf.TFModel._make_inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': (13, ),\n",
    "    'labels/shape': (1, )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other model configuration parameters include loss function, optimizer, and model specific parameters (number of blocks, a type of activation function, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'features',\n",
    "    'body/units': 1,\n",
    "    'loss': 'mse',\n",
    "    'optimizer': {'name':'Adam'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config keys:\n",
    "* __inputs__ - input data configuration (described above).\n",
    "* __initial_block/inputs__ - the name of input tensor which should be fed into the model.\n",
    "* __body/units__ - number of neurons in fully-connected layers. Can be a _list_, if you have more than one dense layer.\n",
    "* __loss__ - a loss function to optimize.\n",
    "* __optimizer__ - an optimization algorithm and its parameters.\n",
    "\n",
    "As you can see, some configuration options have a hierarchical structure. See the documentationfor more info about [the models](https://analysiscenter.github.io/batchflow/intro/tf_models#getting-started) and [model configration](https://analysiscenter.github.io/batchflow/intro/models#configuring-a-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    'features': B('features'),\n",
    "    'labels': B('labels')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__feed_dict__ is a dict in which:\n",
    "* _key_ is a placeholder name\n",
    "* _value_ is input data for that placeholder.\n",
    "\n",
    "Letter B in _values_ is a [named expression](https://analysiscenter.github.io/batchflow/intro/named_expr.html) which replaces the name within it into a value from a batch class attribute or a component.\n",
    "\n",
    "In this case, it is the [component](https://analysiscenter.github.io/batchflow/intro/batch.html#components) name defined in the batch class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create a pipeline, which can generate batches and train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_linear = (linear_dset.train.p\n",
    "                .load(src=(linear_x, linear_y))\n",
    "                .init_model('dynamic',\n",
    "                            TorchRegressionModel,\n",
    "                            #TFRegressionModel,\n",
    "                            name='linear',\n",
    "                            config=config)\n",
    "                .train_model('linear', features=B('features'), labels=B('labels'), fetches='loss')\n",
    "                .run_later(BATCH_SIZE, shuffle=True, n_epochs=10, bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██▏       | 17/80 [00:00<00:00, 164.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▋     | 37/80 [00:00<00:00, 172.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 80/80 [00:00<00:00, 200.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7faabc05b6d8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_linear.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction pipeline would be also helpful. For this purpose the method [__predict_model__](https://analysiscenter.github.io/batchflow/intro/models.html#predicting-with-a-model) is used. \n",
    "\n",
    "I would direct your attention to the argument named __fetches__.\n",
    "It returns a value of tensor with a specified name. Using it, you can always get from model any tensor you want.\n",
    "\n",
    "As you might already know, in __output__ configuration parameter (when calling __init_model__) you can specify a list of useful outputs, such as 'proba', 'sigmoid', etc. (if you don't know about it, read [the documentation](https://analysiscenter.github.io/batchflow/intro/tf_models#output)). And __predict_model__'s __fetches__ argument allows to get those outputs from a model.\n",
    "\n",
    "Another important method is [__import_model__](https://analysiscenter.github.io/batchflow/intro/models.html#importing-models) that loads a model from another pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d7a987452838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                 \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                 save_to=V('predict', mode='a'))\n\u001b[0;32m---> 11\u001b[0;31m                  .run(BATCH_SIZE, shuffle=False, n_epochs=1, bar=True))\n\u001b[0m",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/pipeline.py\u001b[0m in \u001b[0;36m_gen_batch\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m                     \u001b[0mbatch_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m                         \u001b[0mupdate_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/pipeline.py\u001b[0m in \u001b[0;36mexecute_for\u001b[0;34m(self, batch, new_loop)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mbatch_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_all_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0mbatch_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/pipeline.py\u001b[0m in \u001b[0;36m_exec_all_actions\u001b[0;34m(self, batch, actions)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0maction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mjoin_batches\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/pipeline.py\u001b[0m in \u001b[0;36m_exec_predict_model\u001b[0;34m(self, batch, action)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_model_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_to'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/models/torch/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, targets, feed_dict, train_mode, fetches, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`targets` already present in `feed_dict`, so those passed as keyword arg won't be used\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/models/torch/base.py\u001b[0m in \u001b[0;36m_fill_input\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_names\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fill_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/models/torch/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_names\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fill_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/models/torch/base.py\u001b[0m in \u001b[0;36m_fill_param\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/kalashnikov/SeismicPro/seismicpro/batchflow/batchflow/models/torch/base.py\u001b[0m in \u001b[0;36m_fill_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fill_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "test_linear = (linear_dset.test.p\n",
    "                 .load(src=(linear_x, linear_y))\n",
    "                 .import_model('linear', train_linear)\n",
    "                 .init_variable('predict', [])\n",
    "                 .predict_model('linear',\n",
    "                                #B('features'), \n",
    "                                fetches='predictions',\n",
    "                                features=B('features'),    \n",
    "                                labels=B('labels'),\n",
    "                                save_to=V('predict', mode='a'))\n",
    "                 .run(BATCH_SIZE, shuffle=False, n_epochs=1, bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last __pipeline__, we test our model. Let's see, how well does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = np.array(test_linear.get_variable('predict')).reshape(-1, 1)\n",
    "target = np.array(linear_y[linear_dset.test.indices])\n",
    "\n",
    "error = np.mean(np.abs((target - predict) * 100 / target))\n",
    "\n",
    "print('Average error: {}%'.format(round(error, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is far from being perfect because the training was too short (in order not make you wait too long). Increase the number of epochs in the training pipeline up to 1000 to get a more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression \n",
    "It solves a task with a binary target (0 or 1, -1 or 1 and etc).\n",
    "\n",
    "To train a logistic regression we generate two-dimensional data from two linearly separable clusters and fit a model to predict a cluster for a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logistic_data(size, first_params, second_params):\n",
    "    \"\"\" Generation of data for fit logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        number of data items\n",
    "\n",
    "    first_params: list of list\n",
    "        distribution params for cloud #0\n",
    "\n",
    "    second_params: list of list\n",
    "        distribution params for cloud #1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        coordinates in two-dimensional space\n",
    "\n",
    "    y: numpy array\n",
    "        labels {0, 1}\n",
    "    \"\"\"\n",
    "    first = np.random.multivariate_normal(first_params[0], first_params[1], size)\n",
    "    second = np.random.multivariate_normal(second_params[0], second_params[1], size)\n",
    "\n",
    "    x = np.vstack((first, second))\n",
    "    y = np.hstack((np.zeros(size), np.ones(size)))\n",
    "    shuffle = np.arange(len(x))\n",
    "    np.random.shuffle(shuffle)\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle] #.reshape(-1, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500\n",
    "logistic_x, logistic_y = generate_logistic_data(size, [[1,2],[[15,0],[0,15]]], [[10,17],[[15,0],[0,15]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-poster')\n",
    "plt.style.use('ggplot')\n",
    "plt.scatter(logistic_x[:,0], logistic_x[:,1], c=logistic_y)\n",
    "plt.title('Cloud points distribution', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important things that you need to know is that it really doesn't matter which model you want to train and what data you will use for it. The procedure stays the same.\n",
    "\n",
    "First of all, a dataset is created and split into train/test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_dset = Dataset(size, batch_class=MyBatch)\n",
    "logistic_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pipeline and configurations also does not change much:\n",
    "\n",
    "- __shape__ has changed from 13 to 2 since now we have two-dimensional data. \n",
    "\n",
    "- for __labels__ new parameters appeared:\n",
    "    * __classes__ - the number of classes. We have a binary classification, hence 2 classes. \n",
    "    * __transform__ - type of transformation for labels. We use one hot encoding or 'ohe'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': 2,\n",
    "    'labels/classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and execute the pipeline with __run__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_logistic = (logistic_dset.train.p\n",
    "                .load(src=(logistic_x, logistic_y))\n",
    "                .init_variable('loss_history', init_on_each_run=list)\n",
    "                .init_model('dynamic',\n",
    "                            RegressionModel,\n",
    "                            'logistic',\n",
    "                            config={\n",
    "                                'inputs': inputs_config,\n",
    "                                'loss': 'ce',\n",
    "                                'optimizer': {'name':'Adam', 'learning_rate': 0.01},\n",
    "                                'initial_block/inputs': 'features',\n",
    "                                'body/units': 2,\n",
    "                                'output': dict(ops=['accuracy'])})\n",
    "                .train_model('logistic',\n",
    "                             fetches='loss',\n",
    "                             feed_dict={\n",
    "                                 'features': B('features'),\n",
    "                                 'labels': B('labels')},\n",
    "                            save_to=V('loss_history', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, create test pipeline and run it too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logistic = (logistic_dset.test.p\n",
    "                .import_model('logistic', train_logistic)\n",
    "                .load(src=(logistic_x, logistic_y))\n",
    "                .init_variables(['predictions', 'metrics'])\n",
    "                .predict_model('logistic', \n",
    "                             fetches='predictions' ,\n",
    "                             feed_dict={\n",
    "                                 'features': B('features'),\n",
    "                                 'labels': B('labels')},\n",
    "                             save_to=V('predictions'))\n",
    "                .gather_metrics(ClassificationMetrics, targets=B('labels'), predictions=V('predictions'),\n",
    "                                fmt='logits', axis=-1, save_to=V('metrics', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=False, n_epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After measure the quality of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_logistic.get_variable('metrics').evaluate('accuracy')\n",
    "print('Percentage of accurate predictions: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again to imporove the accuracy increase the number of epochs in the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson regression \n",
    "Poisson regression is being used if the answers contain counts. \n",
    "\n",
    "The example shows how we can train poisson regression by using data generated from poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(lam, size=10, shape=13):\n",
    "    \"\"\" Generation of data for fit poisson regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        number of data items\n",
    "\n",
    "    lam : float\n",
    "        Poisson distribution parameter\n",
    "    \n",
    "    shape : int\n",
    "        number of features for each data item\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        Matrix with random numbers from the uniform distribution\n",
    "    y: numpy array\n",
    "        random Poisson distributed numbers\n",
    "    \"\"\"\n",
    "    x = np.random.random(size=(size, shape))\n",
    "    b = np.random.random(1)\n",
    "\n",
    "    y_obs = np.random.poisson(np.exp(np.dot(x, lam) + b))\n",
    "\n",
    "    shuffle = np.arange(len(x))\n",
    "    np.random.shuffle(shuffle)\n",
    "    x = x[shuffle]\n",
    "    y = y_obs[shuffle].reshape(-1, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "NUM_DIM = 13\n",
    "poisson_x, poisson_y = generate_poisson_data(np.random.random(NUM_DIM), size, NUM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the same cell as previously, but with different names of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_dset = Dataset(size, batch_class=MyBatch)\n",
    "poisson_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create our own loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_poisson(target, predictions):\n",
    "    loss = tf.reduce_mean(tf.nn.log_poisson_loss(target, predictions))\n",
    "    tf.losses.add_loss(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again __shape__ equals 13, as the shape of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': NUM_DIM,\n",
    "    'labels/shape': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run a train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_poisson = (poisson_dset.train.p\n",
    "                .load(src=(poisson_x, poisson_y))\n",
    "                .init_variable('shape')\n",
    "                .init_model('dynamic', \n",
    "                            RegressionModel, \n",
    "                            'poisson',\n",
    "                            config={\n",
    "                                'inputs': inputs_config,\n",
    "                                'loss': loss_poisson,\n",
    "                                'optimizer': {'name': 'GradientDescent', 'learning_rate': 5e-5},\n",
    "                                'initial_block/inputs': 'features',\n",
    "                                'body/units': 1,\n",
    "                                'output': tf.exp})\n",
    "                .train_model('poisson',\n",
    "                             fetches='loss',\n",
    "                             feed_dict={\n",
    "                                 'features': B('features'),\n",
    "                                 'labels': B('labels')})\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=100, bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test pipeline and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poisson = (poisson_dset.test.p\n",
    "                .load(src=(poisson_x, poisson_y))\n",
    "                .import_model('poisson', train_poisson)\n",
    "                .init_variable('predictions', init_on_each_run=list)\n",
    "                .predict_model('poisson', \n",
    "                               fetches='exp',\n",
    "                               feed_dict={\n",
    "                                   'features': B('features'),\n",
    "                                   'labels': B('labels')},\n",
    "                               save_to=V('predictions', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.array(test_poisson.get_variable('predictions')).reshape(-1, 1)\n",
    "target = np.array(poisson_y[poisson_dset.test.indices])\n",
    "\n",
    "true_var = np.mean((target - np.mean(target))**2)\n",
    "predict_var = np.mean((pred - np.mean(pred))**2)\n",
    "\n",
    "error = np.mean(np.abs(pred - target)) / np.mean(target) * 100\n",
    "print('Average error: {}%'.format(round(error, 3)), 'Variance ratio: %.3f' % (predict_var / true_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* No matter what you want to train and what data you want to use for it, pipeline always looks the same.\n",
    "* It takes time to train an accurate model.\n",
    "* Now you know how to use `batchflow` and specifically how to:\n",
    "    * create pipelines for train and test models\n",
    "    * train linear regression with multi-dimensional data\n",
    "    * train logistic regression (with another type of data - two-dimensions clouds of dots)\n",
    "    * create your own loss function and train poisson regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "You might hone your new skills by:\n",
    "* creating a multi-class regression model;\n",
    "* adding aditional features to the model in order to improve the quality.\n",
    "\n",
    "You might also want to dig deeper into [batch operations](./02_batch_operations.ipynb).\n",
    "\n",
    "Or choose another topic from [the table of contents](./00_content.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
